{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unweighted Averages Over Datasets:\n",
      "----------------------------------------------------------------------\n",
      "Feature                        | Mean Corr |  Std Dev | Datasets\n",
      "----------------------------------------------------------------------\n",
      "IC9600                         |     0.885 |    0.064 |     11.0\n",
      "# of SAM segmentations         |     0.702 |    0.090 |     11.0\n",
      "MSG                            |     0.607 |    0.076 |     11.0\n",
      "M6                             |     0.582 |    0.071 |     11.0\n",
      "symmetry                       |    -0.581 |    0.083 |     11.0\n",
      "edge density                   |     0.566 |    0.078 |     11.0\n",
      "M4                             |    -0.550 |    0.102 |     11.0\n",
      "M1                             |     0.537 |    0.089 |     11.0\n",
      "M7                             |     0.534 |    0.095 |     11.0\n",
      "# of FC-CLIP classes           |     0.526 |    0.175 |     11.0\n",
      "clutter                        |     0.509 |    0.082 |     11.0\n",
      "MUC6                           |     0.483 |    0.077 |     11.0\n",
      "MUC7                           |     0.482 |    0.083 |     11.0\n",
      "MUC8                           |     0.480 |    0.091 |     11.0\n",
      "MUC5                           |     0.475 |    0.080 |     11.0\n",
      "M3                             |    -0.470 |    0.107 |     11.0\n",
      "M10                            |     0.465 |    0.093 |     11.0\n",
      "MUC4                           |     0.464 |    0.088 |     11.0\n",
      "MUC3                           |     0.457 |    0.094 |     11.0\n",
      "MUC2                           |     0.455 |    0.109 |     11.0\n",
      "MUC1                           |     0.370 |    0.130 |     11.0\n",
      "M2                             |    -0.358 |    0.119 |     11.0\n",
      "M11                            |    -0.255 |    0.072 |     11.0\n",
      "M5                             |     0.236 |    0.108 |     11.0\n",
      "gemini surprise score          |     0.217 |    0.145 |      7.0\n",
      "AMNet memorability             |    -0.206 |    0.208 |     11.0\n",
      "M9                             |     0.058 |    0.125 |     11.0\n",
      "\n",
      "Weighted Averages Based on Dataset Sample Sizes:\n",
      "------------------------------------------------------------------------------------------\n",
      "Feature                        |   Weighted Mean | Weighted Std |   Total Samples\n",
      "------------------------------------------------------------------------------------------\n",
      "IC9600                         |           0.899 |        0.060 |        10,474.0\n",
      "# of SAM segmentations         |           0.690 |        0.087 |        10,474.0\n",
      "MSG                            |           0.591 |        0.072 |        10,474.0\n",
      "M6                             |           0.574 |        0.068 |        10,474.0\n",
      "symmetry                       |          -0.561 |        0.078 |        10,474.0\n",
      "edge density                   |           0.550 |        0.078 |        10,474.0\n",
      "M4                             |          -0.537 |        0.099 |        10,474.0\n",
      "M7                             |           0.531 |        0.101 |        10,474.0\n",
      "M1                             |           0.516 |        0.075 |        10,474.0\n",
      "# of FC-CLIP classes           |           0.496 |        0.171 |        10,474.0\n",
      "clutter                        |           0.492 |        0.069 |        10,474.0\n",
      "MUC7                           |           0.465 |        0.075 |        10,474.0\n",
      "MUC8                           |           0.461 |        0.088 |        10,474.0\n",
      "MUC6                           |           0.461 |        0.058 |        10,474.0\n",
      "MUC5                           |           0.450 |        0.055 |        10,474.0\n",
      "M3                             |          -0.439 |        0.077 |        10,474.0\n",
      "MUC4                           |           0.437 |        0.061 |        10,474.0\n",
      "M10                            |           0.434 |        0.062 |        10,474.0\n",
      "MUC3                           |           0.426 |        0.065 |        10,474.0\n",
      "MUC2                           |           0.415 |        0.067 |        10,474.0\n",
      "M2                             |          -0.345 |        0.112 |        10,474.0\n",
      "MUC1                           |           0.326 |        0.087 |        10,474.0\n",
      "M11                            |          -0.240 |        0.070 |        10,474.0\n",
      "M5                             |           0.225 |        0.087 |        10,474.0\n",
      "gemini surprise score          |           0.187 |        0.109 |         5,828.0\n",
      "AMNet memorability             |          -0.181 |        0.201 |        10,474.0\n",
      "M9                             |           0.066 |        0.097 |        10,474.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os\n",
    "\n",
    "def analyze_file_correlations(data, name=None):\n",
    "    \"\"\"Analyze Spearman correlations between 'complexity' and other features in a dataset.\"\"\"\n",
    "    try:\n",
    "        # If 'data' is a file path string, read the CSV\n",
    "        if isinstance(data, str):\n",
    "            df = pd.read_csv(data)\n",
    "        else:\n",
    "            df = data\n",
    "\n",
    "        if 'complexity' not in df.columns:\n",
    "            print(f\"\\nWarning: 'complexity' not found in {name or 'dataset'}\")\n",
    "            return None, 0\n",
    "\n",
    "        correlations = {}\n",
    "        for column in df.columns:\n",
    "            if column not in ['complexity', 'image_id']:\n",
    "                # Compute Spearman correlation (ignoring NaNs)\n",
    "                correlation, _ = stats.spearmanr(df['complexity'], df[column], nan_policy='omit')\n",
    "                if not np.isnan(correlation):\n",
    "                    correlations[column] = correlation\n",
    "\n",
    "        return correlations, len(df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError processing {name or 'dataset'}: {e}\")\n",
    "        return None, 0\n",
    "\n",
    "def aggregate_correlations(all_files):\n",
    "    \"\"\"Aggregate correlations across all individual CSV files.\"\"\"\n",
    "    all_correlations = {}  # Dictionary to collect correlations per feature\n",
    "    all_datasets = []      # List to store (DataFrame, dataset_name) tuples\n",
    "\n",
    "    # Process each individual file\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            dataset_name = os.path.basename(file)\n",
    "            all_datasets.append((df, dataset_name))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    # Iterate over every dataset\n",
    "    for data, name in all_datasets:\n",
    "        correlations, n_samples = analyze_file_correlations(data, name)\n",
    "        if correlations:\n",
    "            for feature, correlation in correlations.items():\n",
    "                # Append a tuple of (correlation, sample size, dataset name) for each feature\n",
    "                all_correlations.setdefault(feature, []).append((correlation, n_samples, name))\n",
    "\n",
    "    # Compute statistics for each feature across datasets\n",
    "    feature_stats = {}\n",
    "    total_datasets = len(all_datasets)\n",
    "    for feature, corr_list in all_correlations.items():\n",
    "        coverage = len(corr_list) / total_datasets\n",
    "        # Extract correlation values and sample sizes\n",
    "        corr_values = [item[0] for item in corr_list]\n",
    "        sample_sizes = [item[1] for item in corr_list]\n",
    "        total_samples = sum(sample_sizes)\n",
    "\n",
    "        # Unweighted (simple) average and standard deviation over datasets\n",
    "        mean_corr = np.mean(corr_values)\n",
    "        std_corr = np.std(corr_values) if len(corr_values) > 1 else 0\n",
    "\n",
    "        # Weighted average and weighted standard deviation\n",
    "        weights = [n / total_samples for n in sample_sizes]\n",
    "        weighted_mean = sum(c * w for c, w in zip(corr_values, weights))\n",
    "        weighted_var = sum(w * (x - weighted_mean) ** 2 for x, w in zip(corr_values, weights))\n",
    "        weighted_std = np.sqrt(weighted_var)\n",
    "\n",
    "        feature_stats[feature] = {\n",
    "            'mean_correlation': mean_corr,\n",
    "            'std_correlation': std_corr,\n",
    "            'weighted_mean_correlation': weighted_mean,\n",
    "            'weighted_std_correlation': weighted_std,\n",
    "            'num_datasets': len(corr_list),\n",
    "            'dataset_coverage': coverage,\n",
    "            'total_samples': total_samples\n",
    "        }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame and add columns for absolute values for sorting\n",
    "    stats_df = pd.DataFrame.from_dict(feature_stats, orient='index')\n",
    "    stats_df['abs_mean_correlation'] = stats_df['mean_correlation'].abs()\n",
    "    stats_df['abs_weighted_mean_correlation'] = stats_df['weighted_mean_correlation'].abs()\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def print_unweighted_features_table(stats_df):\n",
    "    \"\"\"Print table of features using unweighted (simple average) statistics.\"\"\"\n",
    "    # Sort by absolute mean correlation (largest first)\n",
    "    sorted_features = stats_df.sort_values('abs_mean_correlation', ascending=False)\n",
    "    print(\"\\nUnweighted Averages Over Datasets:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Feature':<30} | {'Mean Corr':>9} | {'Std Dev':>8} | {'Datasets':>8}\")\n",
    "    print(\"-\" * 70)\n",
    "    for feature, row in sorted_features.iterrows():\n",
    "        print(f\"{feature[:30]:<30} | {row['mean_correlation']:9.3f} | {row['std_correlation']:8.3f} | {row['num_datasets']:8}\")\n",
    "\n",
    "def print_weighted_features_table(stats_df):\n",
    "    \"\"\"Print table of features using weighted statistics based on dataset sample sizes.\"\"\"\n",
    "    # Sort by absolute weighted mean correlation (largest first)\n",
    "    sorted_features = stats_df.sort_values('abs_weighted_mean_correlation', ascending=False)\n",
    "    print(\"\\nWeighted Averages Based on Dataset Sample Sizes:\")\n",
    "    print(\"-\" * 90)\n",
    "    print(f\"{'Feature':<30} | {'Weighted Mean':>15} | {'Weighted Std':>12} | {'Total Samples':>15}\")\n",
    "    print(\"-\" * 90)\n",
    "    for feature, row in sorted_features.iterrows():\n",
    "        print(f\"{feature[:30]:<30} | {row['weighted_mean_correlation']:15.3f} | {row['weighted_std_correlation']:12.3f} | {row['total_samples']:15,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Example Usage\n",
    "# ----------------------------\n",
    "\n",
    "# List of individual CSV files containing features\n",
    "feature_files = [\n",
    "    \"RSIVL.csv\",\n",
    "    \"VISC.csv\",\n",
    "    \"IC9600 Abstract.csv\",\n",
    "    \"IC9600 Paintings.csv\",\n",
    "    \"IC9600 Scenes.csv\",\n",
    "    \"SAVOIAS Objects.csv\",\n",
    "    \"SAVOIAS Art.csv\",\n",
    "    \"SAVOIAS Scenes.csv\",\n",
    "    \"SAVOIAS Suprematism.csv\",\n",
    "    \"SAVOIAS Interior Design.csv\",\n",
    "    \"IC9600 Advertisement.csv\",\n",
    "    \"IC9600 Architecture.csv\",\n",
    "    \"IC9600 Person.csv\",\n",
    "    \"IC9600 Transport.csv\",\n",
    "    \"IC9600 Objects.csv\",\n",
    "    \"SVG.csv\"\n",
    "]\n",
    "\n",
    "# If your files are in a different folder, set the folder path here\n",
    "features_folder = \"../features\"\n",
    "feature_files = [os.path.join(features_folder, file) for file in feature_files]\n",
    "\n",
    "# Aggregate correlations over the individual files\n",
    "stats_df = aggregate_correlations(feature_files)\n",
    "\n",
    "# Print the two separate tables:\n",
    "print_unweighted_features_table(stats_df)\n",
    "print_weighted_features_table(stats_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation (Permutation Tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
